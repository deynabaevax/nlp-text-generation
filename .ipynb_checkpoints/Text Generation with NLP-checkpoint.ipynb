{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81cfbc5f-c442-487e-a06a-18fc3621d77b",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Generating Text with NLP</h1>\n",
    "    <img width=\"500px\" src=\"https://1000wordphilosophy.files.wordpress.com/2018/02/plato.jpg?w=656&h=458\">\n",
    "</center>\n",
    "\n",
    "# Introduction \n",
    "\n",
    "A language model can predict the probability of the next word in the sequence, based on the words already observed in the sequence. Neural network models are a preferred method for developing statistical language models because they can use a distributed representation, where different words with similar meanings have similar representation and because they can use a large context of recently observed words when making predictions.\n",
    "\n",
    "# Goals\n",
    "\n",
    "    1. Prepare text for developing a word-based language model.\n",
    "    2. Design and fit a neural language model with a learned embedding and an LSTM hidden layer.\n",
    "    3. Use the learned language model to generate new text with similar statistical properties as the source text.\n",
    "\n",
    "# The Dataset\n",
    "The Republic is the classical Greek philosopher Plato's most famous work. It is structured as a dialog on the topic of order and justice within a city state. I got the file from the Project Gutenberg's website. <a href=\"http://www.gutenberg.org/cache/epub/1497/pg1497.txt\">Link</a> to the dataset.\n",
    "\n",
    "# Overview\n",
    "\n",
    "    1. The Data\n",
    "    2. Data Preparation\n",
    "    3. Train the Language Model\n",
    "    4. Use the Language Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f879c3a4-3ecb-4ad0-86e1-df561d7f8d4c",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "The data contains \n",
    "    - chapter heading (e.g. BOOK I)\n",
    "    - many punctuations, (e.g. -, ;, ?, :, etc.)\n",
    "    - long monologues\n",
    "    - quoted dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74b596d6-b384-4815-8f7c-232f29fd41fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from random import randint\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, LSTM, Embedding, Dense, Dropout\n",
    "\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f90186e0-5125-4cbf-8fdd-6296020a65dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the txt file in the memory\n",
    "def load_doc(file_name):\n",
    "    # open the file as read-only\n",
    "    file = open(file_name, \"r\", encoding='utf8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0adfc464-3c84-49a4-88f0-4cc6528ee60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿The Project Gutenberg eBook of The Republic, by Plato\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this eBook or online at\n",
      "www.gutenberg.org. If you are not located in the United States, you\n",
      "will have to check the laws of the country where you are located before\n",
      "using this eBook.\n",
      "\n",
      "Title: The Republic\n",
      "\n",
      "Author: Plato\n",
      "\n",
      "Translator: B. Jowett\n",
      "\n",
      "Release Date: October, 1998 [eBook #1497]\n",
      "[Most recently updated: September 11, 2021]\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "Produced by: Sue Asscher and David Widger\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK THE REPUBLIC ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE REPUBLIC\n",
      "\n",
      "By Plato\n",
      "\n",
      "Trans\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = 'pg1497.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99192393-c69d-4f46-a461-c42b66a5162b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[967, 38188, 553671]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reference: https://pynative.com/python-regex-findall-finditer/\n",
    "# find the beginning of the book\n",
    "[m.start() for m in re.finditer('BOOK I\\.', doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f1c37c4-f94e-428f-8b22-ae0861aa8385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1195178]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the end of the book\n",
    "[m.start() for m in re.finditer('years which we have been describing', doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47d6f972-90d2-470f-830f-6d5f476217a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOK I.\n",
      "\n",
      "\n",
      "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
      "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
      "Artemis.); and also because I wanted to see in wha\n"
     ]
    }
   ],
   "source": [
    "doc = doc[553671:1195178]\n",
    "\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b7c31-d8cc-4eaf-b4c7-74d42956712e",
   "metadata": {},
   "source": [
    "## Cleaning the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f533671a-2b10-4589-baa9-1565aaf796ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replace \"--\" with a space \" \"\n",
    "    doc = doc.replace(\"--\", \" \")\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile(\"[%s]\" % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub(\"\", w) for w in tokens]\n",
    "    # remove the remaining tokens, which are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a2eea2a-ebda-4623-b543-5c981d9d6ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said', 'to']\n",
      "Total Tokens: 117336\n",
      "Unique Tokens: 7323\n"
     ]
    }
   ],
   "source": [
    "# clean doc\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbb50ff-3747-4351-913b-f4119fcae3e6",
   "metadata": {},
   "source": [
    "### Save the Cleaned Text\n",
    "Organize the long list of tokens into sequences of 50 input words and 1 output word. These are sequences of 51 words. A possible way is to iterate over the list of tokens from token 51 onwards and taking the prior 50 tokens as a sequence, then repeating this process to the end of the list of tokens. Then transform the tokens into space-separated strings for later storage in a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce10c547-e76e-4dd0-aa64-eef38d5c9b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 117285\n"
     ]
    }
   ],
   "source": [
    "# organize into sequence of tokens\n",
    "length = 50 + 1\n",
    "seq = list()\n",
    "\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    s = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = \" \".join(s)\n",
    "    # store\n",
    "    seq.append(line)\n",
    "print(\"Total Sequences: %d\" % len(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8081536-ee64-461b-8c3a-dea7151e8cb3",
   "metadata": {},
   "source": [
    "Running the above piece creates a long list of lines. Printing statistics on the list, I can see that I have exactly 117,285 training patterns to fit the model later.\n",
    "\n",
    "\n",
    "Next, I can save the sequences to a new file for later loading. I can define a new function for saving lines of text to a file. This new function is called save_doc() and is listed below. It takes as input a list of lines and a filename. The lines are written, one per line, in ASCII format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16328a23-213d-47bf-96db-86279c27ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to a file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = \"\\n\".join(lines)\n",
    "    file = open(filename, \"w\")\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8e2feef-6f65-4083-ba31-19f996718009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = 'sequences.txt'\n",
    "save_doc(seq, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319df5c5-a05a-4b73-9034-1fde5a0ff9b6",
   "metadata": {},
   "source": [
    "## Prepare the Model for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57cffe02-f3da-499b-86da-560a11c8abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c033414-535e-4aea-aacf-91bb4086477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "in_filename = 'sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f31f113-d2fa-4de8-be84-0a693de325cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
